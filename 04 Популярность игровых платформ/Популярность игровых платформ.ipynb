{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Проект для «Викишоп»"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords as nltk_stopwords\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from IPython.display import display\r\n",
    "from tqdm import notebook\r\n",
    "from sklearn.utils import shuffle\r\n",
    "\r\n",
    "import sys\r\n",
    "if not sys.warnoptions:\r\n",
    "    import warnings\r\n",
    "    warnings.simplefilter(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\r\n",
    "data = data.sample(50000).reset_index(drop=True)\r\n",
    "display(data, data.info())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "text     50000 non-null object\n",
      "toxic    50000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "                                                    text  toxic\n",
       "0      Cleanup tag\\nHi there,\\nDo you have any specif...      0\n",
       "1      \"\\nNo, K-Dee did not found that group. I remov...      0\n",
       "2      This is not a blog. Will both sides please cea...      0\n",
       "3      added section - Medical Practice \\n\\nGosnell's...      0\n",
       "4      School naming \\nAh, I see - Even so, with scho...      0\n",
       "...                                                  ...    ...\n",
       "49995  Getting around the admins and users \\nHi, . I'...      0\n",
       "49996  \"\\nWell, it is an \"\"occupation\"\", just not a g...      0\n",
       "49997  http://www.telegraph.co.uk/culture/music/35634...      0\n",
       "49998  SUPER GAY SUPER GAY SUPER GAY SUPER GAY SUPER ...      1\n",
       "49999  By the way if you just want programmability yo...      0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Cleanup tag\\nHi there,\\nDo you have any specif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"\\nNo, K-Dee did not found that group. I remov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This is not a blog. Will both sides please cea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>added section - Medical Practice \\n\\nGosnell's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>School naming \\nAh, I see - Even so, with scho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49995</td>\n",
       "      <td>Getting around the admins and users \\nHi, . I'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49996</td>\n",
       "      <td>\"\\nWell, it is an \"\"occupation\"\", just not a g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49997</td>\n",
       "      <td>http://www.telegraph.co.uk/culture/music/35634...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49998</td>\n",
       "      <td>SUPER GAY SUPER GAY SUPER GAY SUPER GAY SUPER ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49999</td>\n",
       "      <td>By the way if you just want programmability yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data['toxic'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    44961\n",
       "1     5039\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что пропусков нет, типы данных соответствуют, есть дисбалланс классов."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее все комментарии прогоняются через функцию, которая находит леммы слов и очищает комментарии от знаков пунктуации, смайликов:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\r\n",
    "\r\n",
    "corpus = data['text'].values.astype('U')\r\n",
    "result = []\r\n",
    "def lemmatize(text):\r\n",
    "    for i in notebook.tqdm(text):\r\n",
    "        r = re.sub(r\"[^a-zA-Z ]\",\" \", i)\r\n",
    "        lemm_list = (\" \".join(r.split()))\r\n",
    "        doc = nlp(lemm_list)\r\n",
    "        lemm_text = ' '.join(token.lemma_ for token in doc)\r\n",
    "        result.append(lemm_text)\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data['clear_text'] = lemmatize(corpus)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903c5d110bf94b96a1e2912b129c9c84",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее разбиваю датасет на выборки тренировочных и тестовых признаков и таргетов:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train, test = train_test_split(data, test_size=0.25, random_state=12345)\r\n",
    "tf_idf_train = train['clear_text']\r\n",
    "tf_idf_test = test['clear_text']\r\n",
    "target_train = train['toxic']\r\n",
    "target_test = test['toxic']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выполняю апсемплинг, чтобы уравновесить классы:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "repeat = len(target_train[target_train == 0]) // len(target_train[target_train == 1])\r\n",
    "def upsample(features, target, repeat):\r\n",
    "    features_zeros = features[target == 0]\r\n",
    "    features_ones = features[target == 1]\r\n",
    "    target_zeros = target[target == 0]\r\n",
    "    target_ones = target[target == 1]\r\n",
    "\r\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\r\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\r\n",
    "    \r\n",
    "    features_upsampled, target_upsampled = shuffle(\r\n",
    "        features_upsampled, target_upsampled, random_state=12345)\r\n",
    "    \r\n",
    "    return features_upsampled, target_upsampled\r\n",
    "\r\n",
    "features_upsampled, target_upsampled = upsample(tf_idf_train, target_train, repeat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Расчёт TF-IDF:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "nltk.download('stopwords')\r\n",
    "stopwords = set(nltk_stopwords.words('english'))\r\n",
    "\r\n",
    "tf_idf = TfidfVectorizer(stop_words=stopwords).fit(features_upsampled.values.astype('U'))\r\n",
    "features_train = tf_idf.transform(features_upsampled.values.astype('U'))\r\n",
    "features_test = tf_idf.transform(tf_idf_test.values.astype('U'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вывод"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "В данном блоке я выполнил предобработку данных: лемматизировал текст, очистил от знаков пунктуации, избавился от дисбаланса классов и вычислил TF-IDF."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучаю модель логистической регрессии:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "model_log = LogisticRegression(random_state=12345, solver='saga', C=1.7, multi_class='multinomial')\r\n",
    "model_log.fit(features_train, target_upsampled)\r\n",
    "predict = model_log.predict(features_test)\r\n",
    "print(f1_score(predict, target_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7550713749060857\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее гридсёрчем перебираю параметры для модели дерева решений:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "gridParams = {\r\n",
    "    'max_depth':range(307,308)\r\n",
    "    }\r\n",
    "\r\n",
    "model_tree =  DecisionTreeClassifier(random_state=12345, criterion='entropy', splitter='random', min_samples_split=12)\r\n",
    "grid = GridSearchCV(model_tree, gridParams, cv=5, scoring='f1')\r\n",
    "grid.fit(features_train, target_upsampled)\r\n",
    "print(grid.best_params_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'max_depth': 307}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "model_tree =  DecisionTreeClassifier(random_state=12345, max_depth=307, criterion='entropy', splitter='random', min_samples_split=12)\r\n",
    "model_tree.fit(features_train, target_upsampled)\r\n",
    "predict = model_tree.predict(features_test)\r\n",
    "print(f1_score(predict, target_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6787785079242366\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее перебираю параметры для модели случайного леса:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "gridParams = {\r\n",
    "    'max_depth':range(476,477),\r\n",
    "    'n_estimators':range(8,9)\r\n",
    "    }\r\n",
    "\r\n",
    "model_forest =  RandomForestClassifier(random_state=12345, max_features=10000, min_samples_split=3, min_samples_leaf=4)\r\n",
    "grid = GridSearchCV(model_forest, gridParams, cv=5, scoring='f1')\r\n",
    "grid.fit(features_train, target_upsampled)\r\n",
    "print(grid.best_params_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'max_depth': 476, 'n_estimators': 8}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "model_forest =  RandomForestClassifier(random_state=12345, max_depth=476, n_estimators=8, max_features=10000, min_samples_split=3, min_samples_leaf=4)\r\n",
    "model_forest.fit(features_train, target_upsampled)\r\n",
    "predict = model_forest.predict(features_test)\r\n",
    "print(f1_score(predict, target_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7091666666666666\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Выводы"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Лучшее значение показала модель логистической регрессии, далее идёт случайный лес, и хуже всех - дерево решений.\r\n",
    "\r\n",
    "В этот раз моделью логистической регрессии я достиг необходимого значения метрики, но остальные 2 модели так и не дошли до 0.75. Возможно, если бы не падал кернел и удалось лемматизировать весь датасет, то качество моделей еще бы подросло."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Чек-лист проверки"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}